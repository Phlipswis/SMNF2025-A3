---
title: "Aufsatz"
date: today

author:
  - Tara Dethlefsen
  - Jonas Joost
  - Vivien Ramhold
  - Niklas von der Fecht
  - Philipp Wisser

lang: de

format:
  html: default
  pdf:
    papersize: a4
    fontsize: 12pt
    number-sections: true
    
bibliography: quellen.bib
csl: apa.csl

editor: visual
---

**Github Repository und Commit-Hash**

-   **Repository:** \[https://github.com/Phlipswis/SMNF2025-A3\]

```{r}
#| echo: false
gitHash = system("git rev-parse HEAD", intern = TRUE)
```

-   **Commit-Hash:** `{r} gitHash`\

**Code of Conduct**

Mit dem Folgenden haben wir gemeinsame Regeln und Grundsätze formuliert, die unser Miteinander und unsere Arbeitsweise leiten.

Ein zentraler Aspekt ist der respektvolle Umgang mit Feedback sowie unterschiedlichen Perspektiven und Meinungen. Deshalb formulieren wir Kritik konstruktiv mit dem Ziel, gemeinsam eine Lösung zu finden. Die Aufteilung der Arbeitslast erfolgt in unserer Gruppe fair und unter Berücksichtigung der individuellen Fähigkeiten. Jedes Gruppenmitglied übernimmt Verantwortung für seine Aufgaben und trägt so aktiv zum Gelingen des Gesamtprojekts bei. Vereinbarte Termine betrachten wir als verbindlich. Sollte ein Termin aus nachvollziehbaren Gründen nicht eingehalten werden können, informieren wir die Gruppe frühzeitig, damit gemeinsam alternative Lösungen gefunden werden können.

Wir verpflichten uns ehrlich, sorgfältig und transparent zu arbeiten. Plagiate, Datenmanipulation und andere Formen wissenschaftlichen Fehlverhaltens lehnen wir ab. Alle verwendeten Quellen werden ordnungsgemäß zitiert, und die Ergebnisse unserer Arbeit sind jederzeit nachvollziehbar dokumentiert.

Zum Schutz der personenbezogenen und sensiblen Daten gehen wir verantwortungsvoll mit den Informationen um, die uns anvertraut werden. Dabei halten wir uns an die geltenden Datenschutzrichtlinien und sorgen dafür, dass Daten nur für die vorgesehenen Zwecke verwendet und vor unbefugtem Zugriff geschützt werden.

Die Nutzung von Künstlicher Intelligenz (KI) beschränkt sich auf die Korrektur von Rechtschreibung und Grammatik sowie das Generieren von R- und Quarto-Code. In allen anderen Fällen wird die Nutzung von KI-Tools gemäß der Richtlinien der American Psychological Association in der 7. Auflage kenntlich gemacht.

# Einleitung

*Einleitung*

# Literaturübersicht

Misinformation stellt ein zentrales Problem auf Social Media dar. Insbesondere seit den Wahlen in den Vereinigten Staaten im Jahr 2016 ist das Interesse gestiegen, diese Misinformationen zu detektieren. Angesichts der Informationsflut ist eine manuelle Verifikation jedoch nicht flächendeckend umsetzbar. KI-gestützte Systeme bieten durch automatisierte Faktenprüfung die Möglichkeit, schneller und umfangreicher zu arbeiten. Neben der automatisierten Faktenprüfung gibt es auch KI-Methoden der Sprach- und Sentimentanalyse. Die Art des KI-Einsatzes ist entscheidend für den Erfolg der Erkennung von Misinformationen [@santos_artificial_2023].

Gondwe [-@gondwe_can_2025] analysierte vor diesem Hintergrund verschiedene KI-Modelle zur Erkennung von Misinformation bezogen auf die erste US-Präsidentschaftsdebatte im Jahr 2024. Deep-Learning-Ansätze wie Bidirectional encoder representations from transformers erreichten die höchste Genauigkeit mit 94,8 Prozent. Diese Methoden sind jedoch aufgrund des hohen Rechenaufwands schwer skalierbar und demnach nicht in Echtzeit realisierbar. Andere Modelle mit geringerem Rechenaufwand machen hingegen Abstriche in der Genauigkeit [@gondwe_can_2025]. Es stellt sich demnach die Frage, welcher Aspekt für die Nutzer bezogen auf die KI als Werkzeug überwiegt.

Ein weiterer Aspekt für den Erfolg der Erkennung von Misinformationen mit Hilfe von KI ist die zeitliche Einbettung. Qin et al. [-@qin_timing_2025] zeigten in einer Studie, dass der Einsatz von KI die Kreativität der Nutzenden beschränkt, sofern zuvor keine eigenen Ideen entwickelt wurden. Diesen Befund wollen wir auf die Bekämpfung von Misinformationen übertragen. Wir vermuten, dass auch hier die zeitliche Einbettung der KI für den Erfolg entscheidend ist.

Nicht nur technische, sondern auch soziale Einflüsse sind wichtig zu betrachten. Lu et al. [-@lu_effects_2022] konnten zeigen, dass Nutzer KI-generiertem Feedback insbesondere dann vertrauen, wenn soziale Hinweise fehlen. Unter sozialem Druck relativiert sich dieses Vertrauen, was die Komplexität der Nutzer-KI-Interaktion unterstreicht. Auf Social Media könnten Kommentare demnach die KI-Einordnung von Misinformationen überschatten.

Alle genannten Studien zeigen, dass KI wirksam gegen Misinformationen auf Social Media eingesetzt werden kann. Die Effektivität hängt jedoch von mehreren Rahmenbedingungen ab. Dazu zählt die unterschiedliche Genauigkeit der Systeme, der Zeitpunkt des Einsatzes und der soziale Kontext. Diese Faktoren wollen wir im Extended Abstract weiter analysieren.

# Methode

## Qualitative Methode

Für die Beantwortung unserer Forschungsfrage entschieden wir uns für einen qualitativen Ansatz, denn wir verfolgen das Ziel, individuelle Erfahrungen und Sichtweisen detailliert zu erfassen und zu verstehen. Im Mittelpunkt steht dabei die Frage, welche Nutzungsanforderungen an KI als Werkzeug zur Unterstützung bei der Detektion von Misinformation auf Social Media bestehen.

Zur Datenerhebung führten wir jeweils ein leitfadengestütztes Interview mit insgesamt drei Teilnehmenden durch. Die Rekrutierung der Teilnehmenden erfolgte gezielt durch persönliche Ansprache. Dabei achteten wir darauf, dass sie über grundlegende Kenntnisse im Umgang mit KI verfügten. Die Interviews wurden entweder online mit Aufzeichnung über OBS-Studio durchgeführt oder vor Ort unter Verwendung eines Smartphone-Diktiergeräts. Vor Beginn der Interviews unterzeichneten alle Teilnehmenden eine Einwilligungserklärung mit Informationen zur Studie, Datenschutz und anonymisierter Auswertung. Die Gespräche wurden anschließend mit der Transkriptionssoftware Whisper verschriftlicht, folgend händisch überarbeitet und anonymisiert.

Für die Analyse verwendeten wir die Thematische Analyse nach Braun und Clarke [-@braun_using_2006]. Ziel war es, aus dem Material übergeordnete Themen zu extrahieren, die zentrale Anforderungen der Nutzenden widerspiegeln. Die Analyse wurde manuell in Microsoft Word durchgeführt und erfolgte induktiv. Zunächst analysierten wir die Transkripte unabhängig voneinander. Anschließend wurden die identifizierten Codes und Themen im Abgleich zusammengeführt. Bei abweichenden Codierungen wurden die Unterschiede besprochen und schließlich zu einer validen und konsistenten Kategorisierung zusammengefügt. Durch dieses Vorgehen wurde die Subjektivität einzelner Interpretationen reduziert und eine möglichst transparente Auswertung gewährleistet.

## Quantitative Methode

Im quantitativen Teil unserer Studie führten wir eine Online-Umfrage durch. Diese bestand aus einem Between-Subjects Design, welches den Unterschied zwischen einer evaluativen KI und einer empfehlenden KI untersucht. Sowie aus einem Within-Subject Design, um den Unterschied zwischen der Bewertung von Social Media Posts mit und ohne Unterstützung einer KI zu untersuchen. Es handelt sich bei unserer quantitativen Methode demnach um ein Mixed Design.

Die Rekrutierung erfolgte über einen schriftlichen Aufruf, der einen Link zur Umfrage enthielt (siehe [Anhang 1](#anhang-rekrutierung)). Den Text verbreiteten wir im Bekanntenkreis, sowie im „Studien und Umfragen“-Forum des Allgemeinen Studierendenausschusses der Universität zu Lübeck. Die Teilnehmenden mussten mindestens 18 Jahre alt sein, um den Datenschutz gemäß der Datenschutz-Grundverordnung gewährleisten zu können. Zudem setzten wir gute Deutschkenntnisse voraus, da unsere Umfrage ausschließlich auf Deutsch verfügbar war und mangelndes Sprachverständnis die Ergebnisse beeinträchtigen könnte. Auf Grund des Interfaces musste die Umfrage an einem PC oder Laptop durchgeführt werden.

Wir setzten uns das Ziel, mindestens 20 ausgefüllte Umfragen zu erhalten, die im Zeitraum vom 25.05.2025 bis zum 02.06.2025 abgeschlossen werden mussten. Ziel der Rekrutierung war eine möglichst heterogene Stichprobe, insbesondere hinsichtlich Alter und Technikaffinität zu erhalten. Studenten der Universität zu Lübeck konnten als Vergütung eine Versuchspersonenstunde erhalten.

Nachdem die Teilnehmenden den Link aus dem Rekrutierungstext aufriefen, wurden sie zunächst durch einen Text über die Studie und den Datenschutz aufgeklärt. Sofern sie diesen zustimmten, startete die mehrteilige Umfrage. Im Pre-Test-Fragebogen wurde die Demografik der Teilnehmer und ihre Technikaffinität abgefragt, sowie ein Tutorial für die anschließenden Aufgaben gezeigt. Die Teilnehmer mussten neben der Bewertung von Social Media Posts einen sekundären Task bearbeiten. Hierbei wurde in der rechten oberen Ecke ein Licht angezeigt, welches zwischen blau und rot wechselte. Sobald das Licht rot schien, musste ein Button in der linken oberen Ecke gedrückt werden. Dieser Task diente zur Messung der mentalen Belastung.

Nach dem Tutorial wurde das Baseline Experiment durchgeführt. Hierfür mussten alle Teilnehmenden 10 Posts ohne die Unterstützung von KI bewerten. Sie gaben an, ob sie den Post als Misinformation einschätzten und wie sicher sie sich mit dieser Entscheidung waren. Anschließend folgte ein Post-Baseline-Fragebogen, der das subjektive Workload erfasste. Im nächsten Schritt wurden die Befragten randomisiert einem evaluativen oder empfehlenden Systemtyp zugeordnet. Es mussten erneut 10 Posts bewertet werden, diesmal jedoch mit der Unterstützung des jeweiligen KI-Typen. Bei jedem Post mussten die Teilnehmenden mindestens eine Frage an die KI stellen. Sie konnten jedoch auch mehrere Fragen stellen und mussten währenddessen weiterhin den sekundären Task ausführen. Nach der Bewertung der 10 Posts musste ein weiterer Fragebogen beantwortet werden, dieser beinhaltete Fragen zum Workload, zur User Experience und zum Vertrauen in das System. Die Studie endete mit einem kurzen Debriefing. Der gesamte Ablauf der Studie ist in Diagramm @fig-ablauf veranschaulicht.

Zur Erfassung des Workloads verwendeten wir das DLR Workload Assessment Tool nach [@ev_dlr-wat_2018]. Die Skala besteht aus acht Item, mit denen die subjektive Einschätzung der momentanen Beanspruchung abgefragt werden.
Die Teilnehmer müssen die einzelnen Items auf einer numerischen Skala von 0 bis 200 bewerten, welche von sehr stark unterbeansprucht (0) über optimal (100) bis sehr stark überbeansprucht (200) geht. Die Subskalen werden zu einem Gesamtwert des Workloads aufsummiert. Die User Experience fragten wir mit Hilfe des User Experience Questionaire nach [@laugwitz_construction_2008] ab. Der Fragebogen besteht aus 26 Items, welche sechs Dimensionen der User Experience behandeln: Attraktivität, Durchschaubarkeit, Originalität, Stimulation, Steuerbarkeit, Effizienz.

Die einzelnen Items bestehen jeweils aus einem Paar von Eigenschaften, welche sich auf einer 7-Punkte-Skala gegenüberstehen. Ein Beispiel für ein solches Item ist langweilig (1) bis spannend (7). Für jede der sechs Dimensionen wird der Mittelwert der zugehörigen Items berechnet. Um das Vertrauen in das System zu untersuchen, verwendeten wir eine ins Deutsche übersetzte Version der Human-Computer Trust 2 (HCT2) Skala nach [@madsen_measuring_2000]. In dieser Studie nutzten wir folgende drei Dimensionen: Perceived Reliability zu Deutsch Wahrgenommene Zuverlässigkeit, Perceived Technical Competence zu Deutsch Wahrgenommene technische Kompetenz und Perceived Understandability zu Deutsch Wahrgenommene Verständlichkeit. Jede Dimension wurde durch fünf Items operationalisiert. Die Teilnehmenden müssen jede Aussage auf einer sechs-stufigen Likert-Skala von stimmt gar nicht (1) bis stimmt völlig (6) bewerten. Anschließen wird ebenfalls für jede Dimension der Mittelwert der zugehörigen Items berechnet.

Wir werden die gesammelten Daten unter Anwendung verschiedener Tests analysieren.
Zum einen möchten wir klären, ob sich der Workload bei Unterstützung durch KI signifikant von dem ohne KI-Einsatz unterscheidet. Diese Fragestellung überprüft das Ergebnis der qualitativen Analyse, wonach der Einsatz von KI die Erkennung von Misinformationen für Benutzer erleichtert. Um dies zu testen, führen wir einen T-Test für abhängige Stichproben durch, wobei der KI-Einsatz als unabhängige Variable und der Workload als abhängige Variable dient.

Des Weiteren werden wir untersuchen, ob sich die User Experience mit einer evaluativen KI signifikant von der mit einer empfehlenden KI unterscheidet. Für jede Subskala des User Experience Questionaire werden wir einen separaten T-Test für unabhängige Stichproben durchführen. Die Subskalen stellen demnach jeweils die abhängige und der KI-Systemtyp die unabhängige Variable dar. In unserem letzten Test wollen wir die Frage untersuchen, in welchem Zusammenhang die Anzahl geschickter Nachrichten mit dem Vertrauen in das System steht.
Diese Frage basiert auf dem Ergebnis der qualitativen Analyse, dass Nutzer der KI mehr Vertrauen schenken, wenn sie den Analyseweg nachvollziehen können. Für diesen Test verwenden wir je Dimension der HCT2-Skala die Pearson-Korrelation, da wir aus den gesendeten Nachrichten der Teilnehmer jeweils einen Mittelwert bilden.


![Ablaufdiagram der Studie](src/mermaid_survey_flowchart.png){#fig-ablauf}

# Ergebnisse

## Qualitative Ergebnisse

Unsere qualitative Analyse besteht aus drei Einzelinterviews mit Studierenden der Medieninformatik an der Universität zu Lübeck. Hierbei wurden die Personen nach ihren Einschätzungen zu der Nutzung von KI bei der Erkennung von Misinformationen auf sozialen Medien befragt. Ziel war es, Anforderungen, Erwartungen und potenzielle Probleme solcher Systeme aus Sicht der Nutzenden zu identifizieren. Die zentralen Themen unserer Analyse sind in @tbl-zentrale_themen zusammengefasst und im Folgenden näher beschrieben.

Alle Befragten gaben an, bereits Erfahrungen im Umgang mit KI gesammelt zu haben und soziale Medien überwiegend für Kommunikations- und Unterhaltungszwecke zu nutzen. Zudem sehen sie KI grundsätzlich als ein geeignetes Werkzeug zur automatisierten Faktenprüfung. So äußerte eine Person, dass durch das Abgleichen der Informationen von der KI eine Art Peer Review geschaffen wird und schloss daraus, „dass KI für \[die Detektion von Misinformationen\] absolut geeignet ist“ (TN_3, Zeile 86).

Allerdings wurde mehrfach betont, dass Transparenz eine entscheidende Voraussetzung für das Vertrauen in die KI ist. So sollten Quellen offengelegt werden, „dass man dann auch nachverfolgen kann, woher die Informationen kommen“ (TN_2, Zeile 99–100) „und vielleicht auch den Analyseweg“ (TN_1, Zeile 102-103). Den Nutzenden ist es demnach wichtig, dass im Zweifelsfall zusätzlich zur automatisierten Prüfung durch die KI eine manuelle Kontrolle möglich ist.

Im Hinblick auf die Effektivität der KI-gestützten Überprüfung wurde geäußert, dass die Kennzeichnung der geprüften Inhalte visuell hervorgehoben und schnell erkennbar sein sollte. Eine Person betonte, dass es „auf jeden Fall sehr sichtbar da sein \[sollte\]. Also das heißt, man soll nicht danach suchen müssen“ (TN_3, Z. 218–219). Besonders in sozialen Medien ist eine solche Sichtbarkeit entscheidend, da Nutzende Inhalte meist nur flüchtig wahrnehmen. Eine deutlich erkennbare Markierung kann in diesem Kontext dazu beitragen, die Wirksamkeit der KI-Überprüfung im Alltag zu erhöhen.

In den Interviews zeigte sich deutlich, dass die inhaltliche Richtigkeit der Überprüfungen gegenüber einer schnellen Verfügbarkeit priorisiert wurde. So urteilte eine der Interviewten: „das braucht ein bisschen, aber dafür ist es dann richtig eingeordnet“ (TN_1, Z. 179–180). Diese Einschätzung ist besonders relevant, da verschiedene KI-Modelle jeweils Kompromisse in einer der beiden Kategorien eingehen. Wie bereits in unserer Literaturanalyse dargelegt, erfordern Modelle mit besonders hoher Genauigkeit einen derart hohen Rechenaufwand, dass sie für eine Echtzeit-Anwendung ungeeignet sind [@gondwe_can_2025]. Das Ergebnis unserer Interviews ist jedoch, dass eine hohe inhaltliche Präzision gegenüber einer sofortigen Einordnung klar bevorzugt wird.

```{r tbl-zentrale_themen, results='asis'}
#| echo: false
library(kableExtra)

df <- data.frame(
  Thema = c("Potenzialbewertung von KI als Werkzeug", 
            "Transparenz durch Quellenangaben", 
            "Sichtbarkeit der Kennzeichnung", 
            "Priorisierung der inhaltlichen Korrektheit"),
  Definition = c("Einschätzung, dass KI bei der Identifikation von Misinformation unterstützen kann", 
                 "Forderung, dass die in der Überprüfung verwendeten Quellen offengelegt werden, um Transparenz zu gewährleisten", 
                 "Forderung, dass die Kennzeichnung der KI-Überprüfung visuell hervorgehoben wird, um Effektivität zu gewährleisen", 
                 "Bewertung, dass die inhaltliche Richtigkeit der Überprüfung wichtiger ist als deren schnelle Verfügbarkeit"),
  Zitat = c("„[…] Peer Review quasi dadurch zu schaffen […] also glaube ich schon sehr gut, dass KI für sowas absolut geeignet ist“ (TN_3, Zeile 83–86)", 
            "„Die Quellenangabe, dass man dann auch nachverfolgen kann, woher die Informationen kommen“ (TN_2, Zeile 99–100)", 
            "„Es sollte auf jeden Fall sehr sichtbar da sein. Also das heißt, man soll nicht danach suchen müssen“ (TN_3, Zeile 218–219)", 
            "„[…] dann speichert es falsch eingeordnet ab. Dann bringt einem das auch nichts, als wenn man sagt, das braucht ein bisschen, aber dafür ist es dann richtig eingeordnet und jeder kann es richtig abspeichern“ (TN_1, Zeile 177–180)")
)

if (knitr::is_latex_output()) {
  kable(df, format = "latex", booktabs = TRUE, caption = "Zentrale Themen der qualitativen Analyse", longtable = TRUE, linesep = "\\addlinespace") %>%
    kable_styling(latex_options = c("hold_position")) %>%
    column_spec(1, width = "4cm") %>%
    column_spec(2, width = "5cm") %>%
    column_spec(3, width = "5cm")
}else if (knitr::is_html_output()) {
  kable(df, format = "html", caption = "Zentrale Themen der qualitativen Analyse", escape = FALSE) %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE) %>%
    column_spec(1, width = "20%") %>%
    column_spec(2, width = "35%") %>%
    column_spec(3, width = "45%")
}
```

# Diskussion

*Diskussion*

# Referenzen {.unnumbered}

::: {#refs}
:::

# Angänge

## Anhang 1 - Rekrutierungstext {#anhang-rekrutierung .unnumbered}

**Online-Studie zur Erkennung von Misinformation auf Social Media**

Hast du schonmal einen Post auf Social Media gesehen und warst dir nicht sicher, ob er wahr oder falsch ist? Misinformation auf Social Media ist oft schwer zu erkennen. In unsere Online-Studie untersuchen wir, inwiefern Künstliche Intelligenz dabei unterstützen kann, solche Inhalte besser zu identifizieren.

Du kannst an der Studie teilnehmen, wenn du:

-   mindestens 18 Jahre alt bist,
-   gut Deutsch sprichst,
-   einen PC oder ein Tablet zur Verfügung hast (keine Smartphone-Teilnahme möglich),
-   und etwa 60 Minuten Zeit mitbringst.

**Teilnahmelink:**

https://dsslab.hciuse.sh/study/pilot?groupId=gr-a3

*(Bitte im Browser auf einem PC oder Tablet öffnen.)*

**Vergütung:** Studierende der Medieninformatik oder Psychologie an der Universität zu Lübeck können 1 VP-Stunde erhalten. Wichtig: Teilnehmende an der Veranstaltung SMNF dürfen die Studie nicht bearbeiten.
